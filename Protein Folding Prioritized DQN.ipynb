{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein Folding Prioritized DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "if USE_CUDA:\n",
    "    print(\"Using GPU: GPU requested and available.\")\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    dtypelong = torch.cuda.LongTensor\n",
    "else:\n",
    "    print(\"NOT Using GPU: GPU not requested or not available.\")\n",
    "    dtype = torch.FloatTensor\n",
    "    dtypelong = torch.LongTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, q_network, target_q_network):\n",
    "        self.env = env\n",
    "        self.q_network = q_network\n",
    "        self.target_q_network = target_q_network\n",
    "        self.num_actions = env.action_space.n\n",
    "\n",
    "    def act(self, state, epsilon):\n",
    "        \"\"\"DQN action - max q-value w/ epsilon greedy exploration.\"\"\"\n",
    "        if random.random() > epsilon:\n",
    "            state = torch.tensor(np.float32(state)).type(dtype).unsqueeze(0)\n",
    "            q_value = self.q_network.forward(state)\n",
    "            return q_value.max(1)[1].data[0]\n",
    "        return torch.tensor(random.randrange(self.env.action_space.n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prioritized Replay Buffer\n",
    "\n",
    "Prioritized Experience Replay: https://arxiv.org/abs/1511.05952"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaivePrioritizedBuffer(object):\n",
    "    def __init__(self, capacity, prob_alpha=0.6):\n",
    "        self.prob_alpha = prob_alpha\n",
    "        self.capacity   = capacity\n",
    "        self.buffer     = []\n",
    "        self.pos        = 0\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        assert state.ndim == next_state.ndim\n",
    "        state      = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "        \n",
    "        max_prio = self.priorities.max() if self.buffer else 1.0\n",
    "        \n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append((state, action, reward, next_state, done))\n",
    "        else:\n",
    "            self.buffer[self.pos] = (state, action, reward, next_state, done)\n",
    "        \n",
    "        self.priorities[self.pos] = max_prio\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:self.pos]\n",
    "        \n",
    "        probs  = prios ** self.prob_alpha\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        \n",
    "        total    = len(self.buffer)\n",
    "        weights  = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        weights  = np.array(weights, dtype=np.float32)\n",
    "        \n",
    "        batch       = list(zip(*samples))\n",
    "        states      = np.concatenate(batch[0])\n",
    "        actions     = batch[1]\n",
    "        rewards     = batch[2]\n",
    "        next_states = np.concatenate(batch[3])\n",
    "        dones       = batch[4]\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones, indices, weights\n",
    "    \n",
    "    def update_priorities(self, batch_indices, batch_priorities):\n",
    "        for idx, prio in zip(batch_indices, batch_priorities):\n",
    "            self.priorities[idx] = prio\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_size = 100000\n",
    "replay_buffer = NaivePrioritizedBuffer(replay_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beta Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_start = 0.4\n",
    "beta_frames = 1000 \n",
    "beta_by_frame = lambda frame_idx: min(1.0, beta_start + frame_idx * (1.0 - beta_start) / beta_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([beta_by_frame(i) for i in range(10000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon Greedy Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 500\n",
    "\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([epsilon_by_frame(i) for i in range(10000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Temporal Difference Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss(agent, batch_size, replay_buffer, optimizer, gamma, beta):\n",
    "    state, action, reward, next_state, done, indices, weights = replay_buffer.sample(batch_size, beta)\n",
    "    \n",
    "    state = torch.tensor(np.float32(state)).type(dtype)\n",
    "    next_state = torch.tensor(np.float32(next_state)).type(dtype)\n",
    "    action = torch.tensor(action).type(dtypelong)\n",
    "    reward = torch.tensor(reward).type(dtype)\n",
    "    done = torch.tensor(done).type(dtype)\n",
    "    weights = torch.tensor(weights).type(dtype)\n",
    "\n",
    "    q_values      = agent.q_network(state)\n",
    "    next_q_values = agent.target_q_network(next_state)\n",
    "\n",
    "    q_value          = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    next_q_value     = next_q_values.max(1)[0]\n",
    "    expected_q_value = reward + gamma * next_q_value * (1 - done)\n",
    "    \n",
    "    loss  = (q_value - expected_q_value.detach()).pow(2) * weights\n",
    "    prios = loss + 1e-5\n",
    "    loss  = loss.mean()\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    replay_buffer.update_priorities(indices, prios.data.cpu().numpy())\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update target network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(q_network, target_q_network, tau):\n",
    "    for t_param, param in zip(target_q_network.parameters(), q_network.parameters()):\n",
    "        if t_param is param:\n",
    "            continue\n",
    "        new_param = tau * param.data + (1.0 - tau) * t_param.data\n",
    "        t_param.data.copy_(new_param)\n",
    "\n",
    "def hard_update(q_network, target_q_network):\n",
    "    for t_param, param in zip(target_q_network.parameters(), q_network.parameters()):\n",
    "        if t_param is param:\n",
    "            continue\n",
    "        new_param = param.data\n",
    "        t_param.data.copy_(new_param)\n",
    "        \n",
    "def update_target(q_network, target_q_network):\n",
    "    target_q_network.load_state_dict(q_network.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "target_update_rate = 0.1\n",
    "gamma = 0.99\n",
    "target_network_update_f = 1000\n",
    "num_timesteps = 10000\n",
    "log_every = 200\n",
    "batch_size = 32\n",
    "start_train = 32\n",
    "\n",
    "def train(env):\n",
    "    \n",
    "    agent = Agent(env, q_network, target_q_network)\n",
    "    optimizer = optim.Adam(q_network.parameters(), lr = learning_rate)\n",
    "\n",
    "    losses, all_rewards = [], []\n",
    "    episode_reward = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    for ts in range(1, num_timesteps + 1):\n",
    "        epsilon = epsilon_by_frame(ts)\n",
    "        action = agent.act(state, epsilon)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(int(action.cpu()))\n",
    "        \n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            all_rewards.append(episode_reward)\n",
    "            episode_reward = 0\n",
    "\n",
    "        if len(replay_buffer) > start_train:\n",
    "            beta = beta_by_frame(ts)\n",
    "            loss = compute_td_loss(agent, batch_size, replay_buffer, optimizer, gamma, beta)\n",
    "            losses.append(loss.data)\n",
    "\n",
    "            if ts % target_network_update_f == 0:\n",
    "                # soft_update(agent.q_network, agent.target_q_network, target_update_rate)\n",
    "                # hard_update(agent.q_network, agent.target_q_network)\n",
    "                update_target(agent.q_network, agent.target_q_network)\n",
    "\n",
    "        if ts % log_every == 0:\n",
    "            out_str = \"Timestep {}\".format(ts)\n",
    "            if len(all_rewards) > 0:\n",
    "                out_str += \", Reward: {}\".format(all_rewards[-1])\n",
    "            if len(losses) > 0:\n",
    "                out_str += \", TD Loss: {}\".format(losses[-1])\n",
    "            print(out_str)\n",
    "    \n",
    "    return losses, all_rewards, agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Losses and Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(losses, rewards):\n",
    "    plt.figure(figsize = (20,20))\n",
    "    plt.subplot(211)\n",
    "    plt.title(\"Rewards\")\n",
    "    plt.plot(rewards)\n",
    "    plt.subplot(212)\n",
    "    plt.title(\"Loss\")\n",
    "    plt.plot(losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run trained agent on environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(env):\n",
    "    env = env\n",
    "    agent = Agent(env, q_network, target_q_network)\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action = agent.act(state, 0)\n",
    "        next_state, reward, done, info = env.step(int(action.cpu()))\n",
    "        env.render()\n",
    "        state=next_state\n",
    "        if done:\n",
    "            print(\"Reward: {} | Actions: {}\".format(reward, info['actions']))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prioritized DQN with Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lattice2d_linear_env import Lattice2DLinearEnv\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_inputs[0], 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Lattice2DLinearEnv(\"H\")\n",
    "q_network = DQN(env.observation_space.shape, env.action_space.n)\n",
    "target_q_network = DQN(env.observation_space.shape, env.action_space.n)\n",
    "\n",
    "if USE_CUDA:\n",
    "    q_network = q_network.cuda()\n",
    "    target_q_network = target_q_network.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on single sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Lattice2DLinearEnv(\"HPPHPHPH\")\n",
    "train(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_agent(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on multiple sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Env params\n",
    "collision_penalty = -2\n",
    "trap_penalty = 0.5\n",
    "\n",
    "max_seq_length = 5\n",
    "seq_dict = {}\n",
    "\n",
    "# Train on all sequences with length <= max_seq_length\n",
    "for seq in seqs_list:\n",
    "    if len(seq) > max_seq_length:\n",
    "        break\n",
    "    else:\n",
    "        env = Lattice2DLinearEnv(seq, collision_penalty, trap_penalty)\n",
    "        # Brute force for sequences with length less than 4\n",
    "        if len(seq) <= 4:\n",
    "            reward, actions = env.all_combs()\n",
    "            seq_dict.update( {seq : reward})\n",
    "        else:\n",
    "            losses, rewards, agent = train(env)\n",
    "            seq_dict.update( {seq : rewards[-1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prioritized DQN with CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lattice2d_cnn_env import Lattice2DCNNEnv\n",
    "\n",
    "class CnnDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(CnnDQN, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.num_actions),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def feature_size(self):\n",
    "        return self.features(torch.zeros(1, *self.input_shape)).view(1, -1).size(1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "env = Lattice2DCNNEnv(\"H\")\n",
    "q_network = CnnDQN(env.observation_space.shape, env.action_space.n)\n",
    "target_q_network = CnnDQN(env.observation_space.shape, env.action_space.n)\n",
    "\n",
    "if USE_CUDA:\n",
    "    q_network = q_network.cuda()\n",
    "    target_q_network = target_q_network.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on single sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Lattice2DLinearEnv(\"HPPHPHPH\")\n",
    "train(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_agent(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on multiple sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Env params\n",
    "collision_penalty = -2\n",
    "trap_penalty = 0.5\n",
    "\n",
    "max_seq_length = 5\n",
    "seq_dict = {}\n",
    "\n",
    "# Train on all sequences with length <= max_seq_length\n",
    "for seq in seqs_list:\n",
    "    if len(seq) > max_seq_length:\n",
    "        break\n",
    "    else:\n",
    "        env = Lattice2DLinearEnv(seq, collision_penalty, trap_penalty)\n",
    "        # Brute force for sequences with length less than 4\n",
    "        if len(seq) <= 4:\n",
    "            reward, actions = env.all_combs()\n",
    "            seq_dict.update( {seq : reward})\n",
    "        else:\n",
    "            losses, rewards, agent = train(env)\n",
    "            seq_dict.update( {seq : rewards[-1]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
