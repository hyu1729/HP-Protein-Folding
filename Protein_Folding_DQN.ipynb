{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein Folding DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT Using GPU: GPU not requested or not available.\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "if USE_CUDA:\n",
    "    print(\"Using GPU: GPU requested and available.\")\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    dtypelong = torch.cuda.LongTensor\n",
    "else:\n",
    "    print(\"NOT Using GPU: GPU not requested or not available.\")\n",
    "    dtype = torch.FloatTensor\n",
    "    dtypelong = torch.LongTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, q_network, target_q_network):\n",
    "        self.env = env\n",
    "        self.q_network = q_network\n",
    "        self.target_q_network = target_q_network\n",
    "        self.num_actions = env.action_space.n\n",
    "\n",
    "    def act(self, state, epsilon):\n",
    "        \"\"\"DQN action - max q-value w/ epsilon greedy exploration.\"\"\"\n",
    "        if random.random() > epsilon:\n",
    "            state = torch.tensor(np.float32(state)).type(dtype).unsqueeze(0)\n",
    "            q_value = self.q_network.forward(state)\n",
    "            return q_value.max(1)[1].data[0]\n",
    "        return torch.tensor(random.randrange(self.env.action_space.n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        state = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(\n",
    "            *random.sample(self.buffer, batch_size)\n",
    "        )\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "replay_size = 5000\n",
    "replay_buffer = ReplayBuffer(replay_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon Greedy Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 500\n",
    "\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2718d84f860>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([epsilon_by_frame(i) for i in range(10000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Temporal Difference Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss(agent, batch_size, replay_buffer, optimizer, gamma):\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "    state = torch.tensor(np.float32(state)).type(dtype)\n",
    "    next_state = torch.tensor(np.float32(next_state)).type(dtype)\n",
    "    action = torch.tensor(action).type(dtypelong)\n",
    "    reward = torch.tensor(reward).type(dtype)\n",
    "    done = torch.tensor(done).type(dtype)\n",
    "\n",
    "    # Normal DDQN update\n",
    "    q_values = agent.q_network(state)\n",
    "    q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    # double q-learning\n",
    "    online_next_q_values = agent.q_network(next_state)\n",
    "    _, max_indicies = torch.max(online_next_q_values, dim=1)\n",
    "    target_q_values = agent.target_q_network(next_state)\n",
    "    next_q_value = torch.gather(target_q_values, 1, max_indicies.unsqueeze(1))\n",
    "\n",
    "    expected_q_value = reward + gamma * next_q_value.squeeze() * (1 - done)\n",
    "    loss = (q_value - expected_q_value.data.detach()).pow(2).mean()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the target network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(q_network, target_q_network, tau):\n",
    "    for t_param, param in zip(target_q_network.parameters(), q_network.parameters()):\n",
    "        if t_param is param:\n",
    "            continue\n",
    "        new_param = tau * param.data + (1.0 - tau) * t_param.data\n",
    "        t_param.data.copy_(new_param)\n",
    "\n",
    "\n",
    "def hard_update(q_network, target_q_network):\n",
    "    for t_param, param in zip(target_q_network.parameters(), q_network.parameters()):\n",
    "        if t_param is param:\n",
    "            continue\n",
    "        new_param = param.data\n",
    "        t_param.data.copy_(new_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "target_update_rate = 0.1\n",
    "gamma = 0.99\n",
    "target_network_update_f = 100\n",
    "num_timesteps = 5000\n",
    "log_every = 200\n",
    "batch_size = 32\n",
    "\n",
    "def run_gym(env):\n",
    "    \n",
    "    print(env.seq)\n",
    "    \n",
    "    # Q and target Q networks are global paraneters\n",
    "    agent = Agent(env, q_network, target_q_network)\n",
    "    optimizer = optim.Adam(q_network.parameters(), lr = learning_rate)\n",
    "    \n",
    "    losses, all_rewards = [], []\n",
    "    episode_reward = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    for ts in range(1, num_timesteps + 1):\n",
    "        epsilon = epsilon_by_frame(ts)\n",
    "        action = agent.act(state, epsilon)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(int(action.cpu()))\n",
    "            \n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            all_rewards.append(episode_reward)\n",
    "            episode_reward = 0\n",
    "\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            # Update the q-network & the target network\n",
    "            loss = compute_td_loss(agent, batch_size, replay_buffer, optimizer, gamma)\n",
    "            losses.append(loss.data)\n",
    "\n",
    "            if ts % target_network_update_f == 0:\n",
    "                # soft_update(agent.q_network, agent.target_q_network, target_update_rate)\n",
    "                hard_update(agent.q_network, agent.target_q_network)\n",
    "\n",
    "        if ts % log_every == 0:\n",
    "            out_str = \"Timestep {}\".format(ts)\n",
    "            if len(all_rewards) > 0:\n",
    "                out_str += \", Reward: {}\".format(all_rewards[-1])\n",
    "            if len(losses) > 0:\n",
    "                out_str += \", TD Loss: {}\".format(losses[-1])\n",
    "            print(out_str)\n",
    "        \n",
    "    return losses, all_rewards, agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Loss & Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(losses, rewards):\n",
    "    plt.figure(figsize = (20,20))\n",
    "    plt.subplot(211)\n",
    "    plt.title(\"Rewards\")\n",
    "    plt.plot(rewards)\n",
    "    plt.subplot(212)\n",
    "    plt.title(\"Loss\")\n",
    "    plt.plot(losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run trained agent on environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(agent, env):\n",
    "    env = env\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action = agent.act(state, 0)\n",
    "        next_state, reward, done, info = env.step(int(action.cpu()))\n",
    "        # env.render()\n",
    "        print(action)\n",
    "        state=next_state\n",
    "        if done:\n",
    "            print(\"Reward: {} | Actions: {}\".format(reward, info['actions']))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN with Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lattice2d_linear_env import Lattice2DLinearEnv\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(self.input_shape[0], 64), nn.ReLU(), nn.Linear(64, self.num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "env = Lattice2DLinearEnv(\"H\")\n",
    "q_network = DQN(env.observation_space.shape, env.action_space.n)\n",
    "target_q_network = deepcopy(q_network)\n",
    "\n",
    "if USE_CUDA:\n",
    "    q_network = q_network.cuda()\n",
    "    target_q_network = target_q_network.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on different sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "HHHHH\n",
      "Timestep 200, Reward: -5, TD Loss: 0.01346272137016058\n",
      "Timestep 400, Reward: 1, TD Loss: 0.012972740456461906\n",
      "Timestep 600, Reward: 1, TD Loss: 0.10668357461690903\n",
      "Timestep 800, Reward: 1, TD Loss: 0.00986954290419817\n",
      "Timestep 1000, Reward: 1, TD Loss: 0.0476403646171093\n",
      "Timestep 1200, Reward: 1, TD Loss: 0.16480374336242676\n",
      "Timestep 1400, Reward: 1, TD Loss: 0.11567631363868713\n",
      "Timestep 1600, Reward: 1, TD Loss: 0.1611030548810959\n",
      "Timestep 1800, Reward: 1, TD Loss: 0.006228334736078978\n",
      "Timestep 2000, Reward: 1, TD Loss: 0.13539834320545197\n",
      "Timestep 2200, Reward: 1, TD Loss: 0.09851716458797455\n",
      "Timestep 2400, Reward: 1, TD Loss: 0.13187572360038757\n",
      "Timestep 2600, Reward: 1, TD Loss: 0.04858887195587158\n",
      "Timestep 2800, Reward: 1, TD Loss: 0.1653289496898651\n",
      "Timestep 3000, Reward: 1, TD Loss: 0.12827087938785553\n",
      "Timestep 3200, Reward: 1, TD Loss: 0.024272844195365906\n",
      "Timestep 3400, Reward: 1, TD Loss: 0.06452585011720657\n",
      "Timestep 3600, Reward: 1, TD Loss: 0.007810549344867468\n",
      "Timestep 3800, Reward: 1, TD Loss: 0.11771278828382492\n",
      "Timestep 4000, Reward: 1, TD Loss: 0.22981734573841095\n",
      "Timestep 4200, Reward: 1, TD Loss: 0.026864459738135338\n",
      "Timestep 4400, Reward: 1, TD Loss: 0.0075698476284742355\n",
      "Timestep 4600, Reward: 1, TD Loss: 0.023829836398363113\n",
      "Timestep 4800, Reward: 1, TD Loss: 0.09122278541326523\n",
      "Timestep 5000, Reward: 1, TD Loss: 0.048142723739147186\n",
      "HHHHP\n",
      "Timestep 200, Reward: 0, TD Loss: 0.004575023893266916\n",
      "Timestep 400, Reward: -2, TD Loss: 0.0039043817669153214\n",
      "Timestep 600, Reward: 0, TD Loss: 0.009948205202817917\n",
      "Timestep 800, Reward: -2, TD Loss: 0.0065862517803907394\n",
      "Timestep 1000, Reward: -2, TD Loss: 0.12404461205005646\n",
      "Timestep 1200, Reward: 0, TD Loss: 0.06446030735969543\n",
      "Timestep 1400, Reward: 1, TD Loss: 0.1507667601108551\n",
      "Timestep 1600, Reward: 0, TD Loss: 0.4034281373023987\n",
      "Timestep 1800, Reward: 0, TD Loss: 0.13799558579921722\n",
      "Timestep 2000, Reward: 0, TD Loss: 0.06126350909471512\n",
      "Timestep 2200, Reward: 0, TD Loss: 0.05476788803935051\n",
      "Timestep 2400, Reward: 0, TD Loss: 0.1035815179347992\n",
      "Timestep 2600, Reward: 1, TD Loss: 0.09241184592247009\n",
      "Timestep 2800, Reward: 0, TD Loss: 0.12621217966079712\n",
      "Timestep 3000, Reward: 0, TD Loss: 0.10981889814138412\n",
      "Timestep 3200, Reward: 0, TD Loss: 1.5248630046844482\n",
      "Timestep 3400, Reward: 0, TD Loss: 0.054213523864746094\n",
      "Timestep 3600, Reward: 0, TD Loss: 1.4856700897216797\n",
      "Timestep 3800, Reward: 1, TD Loss: 1.7887086868286133\n",
      "Timestep 4000, Reward: 0, TD Loss: 0.3742940425872803\n",
      "Timestep 4200, Reward: 1, TD Loss: 0.8713052868843079\n",
      "Timestep 4400, Reward: 1, TD Loss: 1.222090244293213\n",
      "Timestep 4600, Reward: 1, TD Loss: 1.4122099876403809\n",
      "Timestep 4800, Reward: 1, TD Loss: 1.7421536445617676\n",
      "Timestep 5000, Reward: 1, TD Loss: 1.6028624773025513\n",
      "HHHPH\n",
      "Timestep 200, Reward: 0, TD Loss: 1.2478468418121338\n",
      "Timestep 400, Reward: -6, TD Loss: 0.13146089017391205\n",
      "Timestep 600, Reward: -2, TD Loss: 0.44856026768684387\n",
      "Timestep 800, Reward: 0, TD Loss: 0.467882364988327\n",
      "Timestep 1000, Reward: 0, TD Loss: 3.162837266921997\n",
      "Timestep 1200, Reward: 0, TD Loss: 1.6615042686462402\n",
      "Timestep 1400, Reward: 0, TD Loss: 0.23375841975212097\n",
      "Timestep 1600, Reward: 0, TD Loss: 0.5321352481842041\n",
      "Timestep 1800, Reward: 0, TD Loss: 1.5333634614944458\n",
      "Timestep 2000, Reward: 0, TD Loss: 0.02573992684483528\n",
      "Timestep 2200, Reward: 0, TD Loss: 1.3681116104125977\n",
      "Timestep 2400, Reward: 0, TD Loss: 1.1804920434951782\n",
      "Timestep 2600, Reward: 0, TD Loss: 1.4521677494049072\n",
      "Timestep 2800, Reward: 0, TD Loss: 0.13408370316028595\n",
      "Timestep 3000, Reward: 0, TD Loss: 0.05986711382865906\n",
      "Timestep 3200, Reward: 0, TD Loss: 0.006663832813501358\n",
      "Timestep 3400, Reward: 0, TD Loss: 0.04948250576853752\n",
      "Timestep 3600, Reward: 0, TD Loss: 0.033552300184965134\n",
      "Timestep 3800, Reward: 0, TD Loss: 0.01067173108458519\n",
      "Timestep 4000, Reward: 0, TD Loss: 0.014359384775161743\n",
      "Timestep 4200, Reward: 0, TD Loss: 0.059544194489717484\n",
      "Timestep 4400, Reward: 1, TD Loss: 0.008571208454668522\n",
      "Timestep 4600, Reward: 1, TD Loss: 0.05793973430991173\n",
      "Timestep 4800, Reward: 1, TD Loss: 0.004197019152343273\n",
      "Timestep 5000, Reward: 1, TD Loss: 0.006295539438724518\n",
      "HHHPP\n",
      "Timestep 200, Reward: -2, TD Loss: 0.025365613400936127\n",
      "Timestep 400, Reward: 0, TD Loss: 0.0998937338590622\n",
      "Timestep 600, Reward: 0, TD Loss: 0.04323963820934296\n",
      "Timestep 800, Reward: -42, TD Loss: 0.038502369076013565\n",
      "Timestep 1000, Reward: 0, TD Loss: 0.04299486428499222\n",
      "Timestep 1200, Reward: -2, TD Loss: 0.021718963980674744\n",
      "Timestep 1400, Reward: 0, TD Loss: 0.02318788319826126\n",
      "Timestep 1600, Reward: 0, TD Loss: 0.018460312858223915\n",
      "Timestep 1800, Reward: 0, TD Loss: 0.060811545699834824\n",
      "Timestep 2000, Reward: 0, TD Loss: 0.07562856376171112\n",
      "Timestep 2200, Reward: 0, TD Loss: 0.21164608001708984\n",
      "Timestep 2400, Reward: 0, TD Loss: 0.04964422062039375\n",
      "Timestep 2600, Reward: 0, TD Loss: 0.0422217920422554\n",
      "Timestep 2800, Reward: 0, TD Loss: 0.023950643837451935\n",
      "Timestep 3000, Reward: 0, TD Loss: 0.013216676190495491\n",
      "Timestep 3200, Reward: 0, TD Loss: 0.027653152123093605\n",
      "Timestep 3400, Reward: 0, TD Loss: 0.03986191377043724\n",
      "Timestep 3600, Reward: 0, TD Loss: 0.020923959091305733\n",
      "Timestep 3800, Reward: 0, TD Loss: 0.045601192861795425\n",
      "Timestep 4000, Reward: 0, TD Loss: 0.015770558267831802\n",
      "Timestep 4200, Reward: 0, TD Loss: 0.1080266609787941\n",
      "Timestep 4400, Reward: 0, TD Loss: 0.023710453882813454\n",
      "Timestep 4600, Reward: 0, TD Loss: 0.004161592107266188\n",
      "Timestep 4800, Reward: 0, TD Loss: 0.003783559426665306\n",
      "Timestep 5000, Reward: 0, TD Loss: 0.0017284480854868889\n",
      "HHPHH\n",
      "Timestep 200, Reward: -5, TD Loss: 0.004904919303953648\n",
      "Timestep 400, Reward: -8, TD Loss: 0.10621347278356552\n",
      "Timestep 600, Reward: -61, TD Loss: 0.05821120738983154\n",
      "Timestep 800, Reward: -2, TD Loss: 0.10222490131855011\n",
      "Timestep 1000, Reward: 0, TD Loss: 0.2783694863319397\n",
      "Timestep 1200, Reward: 0, TD Loss: 0.08526331931352615\n",
      "Timestep 1400, Reward: 1, TD Loss: 1.0009009838104248\n",
      "Timestep 1600, Reward: 1, TD Loss: 0.303930401802063\n",
      "Timestep 1800, Reward: 0, TD Loss: 0.189842090010643\n",
      "Timestep 2000, Reward: 0, TD Loss: 0.3846614956855774\n",
      "Timestep 2200, Reward: 1, TD Loss: 0.025601940229535103\n",
      "Timestep 2400, Reward: 1, TD Loss: 0.4132819175720215\n",
      "Timestep 2600, Reward: 1, TD Loss: 0.05655337870121002\n",
      "Timestep 2800, Reward: 1, TD Loss: 0.07873308658599854\n",
      "Timestep 3000, Reward: 1, TD Loss: 0.07263930141925812\n",
      "Timestep 3200, Reward: 1, TD Loss: 0.14188596606254578\n",
      "Timestep 3400, Reward: 0, TD Loss: 0.0614839531481266\n",
      "Timestep 3600, Reward: 1, TD Loss: 0.04642913118004799\n",
      "Timestep 3800, Reward: 1, TD Loss: 0.04314383864402771\n",
      "Timestep 4000, Reward: 1, TD Loss: 0.020118115469813347\n",
      "Timestep 4200, Reward: 1, TD Loss: 0.02442454732954502\n",
      "Timestep 4400, Reward: 1, TD Loss: 0.027156947180628777\n",
      "Timestep 4600, Reward: 1, TD Loss: 0.019184859469532967\n",
      "Timestep 4800, Reward: 1, TD Loss: 0.04959215223789215\n",
      "Timestep 5000, Reward: 1, TD Loss: 0.5243473052978516\n",
      "HHPHP\n",
      "Timestep 200, Reward: -4, TD Loss: 0.013732886873185635\n",
      "Timestep 400, Reward: 0, TD Loss: 0.011420531198382378\n",
      "Timestep 600, Reward: 0, TD Loss: 0.00935425702482462\n",
      "Timestep 800, Reward: -4, TD Loss: 0.021566765382885933\n",
      "Timestep 1000, Reward: 0, TD Loss: 0.0026956400834023952\n",
      "Timestep 1200, Reward: 0, TD Loss: 0.05727050453424454\n",
      "Timestep 1400, Reward: 1, TD Loss: 0.04081421345472336\n",
      "Timestep 1600, Reward: -2, TD Loss: 0.010413880459964275\n",
      "Timestep 1800, Reward: 0, TD Loss: 0.0999615490436554\n",
      "Timestep 2000, Reward: 0, TD Loss: 0.08731138706207275\n",
      "Timestep 2200, Reward: 0, TD Loss: 0.0716295912861824\n",
      "Timestep 2400, Reward: 0, TD Loss: 0.09662320464849472\n",
      "Timestep 2600, Reward: 0, TD Loss: 0.09953327476978302\n",
      "Timestep 2800, Reward: 0, TD Loss: 0.042271871119737625\n",
      "Timestep 3000, Reward: 0, TD Loss: 0.062279123812913895\n",
      "Timestep 3200, Reward: 0, TD Loss: 0.10001005977392197\n",
      "Timestep 3400, Reward: 0, TD Loss: 0.04008881375193596\n",
      "Timestep 3600, Reward: 0, TD Loss: 0.07062872499227524\n",
      "Timestep 3800, Reward: 0, TD Loss: 0.23738104104995728\n",
      "Timestep 4000, Reward: 0, TD Loss: 0.061521947383880615\n",
      "Timestep 4200, Reward: 0, TD Loss: 0.038059309124946594\n",
      "Timestep 4400, Reward: 0, TD Loss: 0.03970521315932274\n",
      "Timestep 4600, Reward: 0, TD Loss: 0.14105230569839478\n",
      "Timestep 4800, Reward: 0, TD Loss: 0.1058804914355278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 5000, Reward: 0, TD Loss: 0.00608257669955492\n",
      "HHPPH\n",
      "Timestep 200, Reward: 0, TD Loss: 0.6137166023254395\n",
      "Timestep 400, Reward: -2, TD Loss: 0.015301813371479511\n",
      "Timestep 600, Reward: 0, TD Loss: 0.5482757687568665\n",
      "Timestep 800, Reward: -4, TD Loss: 0.08972439169883728\n",
      "Timestep 1000, Reward: 0, TD Loss: 0.06926745176315308\n",
      "Timestep 1200, Reward: 0, TD Loss: 0.030301610007882118\n",
      "Timestep 1400, Reward: 0, TD Loss: 0.03898439556360245\n",
      "Timestep 1600, Reward: 0, TD Loss: 0.023678826168179512\n",
      "Timestep 1800, Reward: 0, TD Loss: 0.08372193574905396\n",
      "Timestep 2000, Reward: 0, TD Loss: 0.13019266724586487\n",
      "Timestep 2200, Reward: 0, TD Loss: 0.0966094583272934\n",
      "Timestep 2400, Reward: 0, TD Loss: 0.015743812546133995\n",
      "Timestep 2600, Reward: 0, TD Loss: 0.18255217373371124\n",
      "Timestep 2800, Reward: 0, TD Loss: 0.09261796623468399\n",
      "Timestep 3000, Reward: 0, TD Loss: 0.0941784605383873\n",
      "Timestep 3200, Reward: 0, TD Loss: 0.11062730103731155\n",
      "Timestep 3400, Reward: 1, TD Loss: 0.15589840710163116\n",
      "Timestep 3600, Reward: 1, TD Loss: 0.05513662099838257\n",
      "Timestep 3800, Reward: 1, TD Loss: 0.08277122676372528\n",
      "Timestep 4000, Reward: 1, TD Loss: 0.0241612009704113\n",
      "Timestep 4200, Reward: 1, TD Loss: 0.00456895912066102\n",
      "Timestep 4400, Reward: 1, TD Loss: 0.02487790584564209\n",
      "Timestep 4600, Reward: 1, TD Loss: 0.007749451790004969\n",
      "Timestep 4800, Reward: 1, TD Loss: 0.0020888622384518385\n",
      "Timestep 5000, Reward: 1, TD Loss: 0.030017569661140442\n",
      "HHPPP\n",
      "Timestep 200, Reward: -2, TD Loss: 0.014128298498690128\n",
      "Timestep 400, Reward: 0, TD Loss: 0.001939290203154087\n",
      "Timestep 600, Reward: 0, TD Loss: 0.013690303079783916\n",
      "Timestep 800, Reward: -2, TD Loss: 0.02227608487010002\n",
      "Timestep 1000, Reward: 0, TD Loss: 0.02758835442364216\n",
      "Timestep 1200, Reward: 0, TD Loss: 0.029039638116955757\n",
      "Timestep 1400, Reward: 0, TD Loss: 0.08927211165428162\n",
      "Timestep 1600, Reward: 0, TD Loss: 0.02558959275484085\n",
      "Timestep 1800, Reward: 0, TD Loss: 0.04699762910604477\n",
      "Timestep 2000, Reward: 0, TD Loss: 0.02022569440305233\n",
      "Timestep 2200, Reward: 0, TD Loss: 0.08097467571496964\n",
      "Timestep 2400, Reward: 0, TD Loss: 0.06550569087266922\n",
      "Timestep 2600, Reward: 0, TD Loss: 0.056172799319028854\n",
      "Timestep 2800, Reward: 0, TD Loss: 0.039144642651081085\n",
      "Timestep 3000, Reward: 0, TD Loss: 0.09768208116292953\n",
      "Timestep 3200, Reward: 0, TD Loss: 0.03472387045621872\n",
      "Timestep 3400, Reward: 0, TD Loss: 0.04531924054026604\n",
      "Timestep 3600, Reward: 0, TD Loss: 0.027713660150766373\n",
      "Timestep 3800, Reward: 0, TD Loss: 0.02549845352768898\n",
      "Timestep 4000, Reward: 0, TD Loss: 0.018250370398163795\n",
      "Timestep 4200, Reward: 0, TD Loss: 0.013268955051898956\n",
      "Timestep 4400, Reward: 0, TD Loss: 0.08580467849969864\n",
      "Timestep 4600, Reward: 0, TD Loss: 0.032754357904195786\n",
      "Timestep 4800, Reward: 0, TD Loss: 0.0015285535482689738\n",
      "Timestep 5000, Reward: 0, TD Loss: 0.002526981756091118\n",
      "HPHHH\n",
      "Timestep 200, Reward: 0, TD Loss: 0.21421393752098083\n",
      "Timestep 400, Reward: -2, TD Loss: 0.11043868213891983\n",
      "Timestep 600, Reward: -2, TD Loss: 0.02475891448557377\n",
      "Timestep 800, Reward: 0, TD Loss: 0.01916961930692196\n",
      "Timestep 1000, Reward: 0, TD Loss: 0.00781122175976634\n",
      "Timestep 1200, Reward: 0, TD Loss: 0.003380696987733245\n",
      "Timestep 1400, Reward: 0, TD Loss: 0.0807676836848259\n",
      "Timestep 1600, Reward: 0, TD Loss: 0.05747643858194351\n",
      "Timestep 1800, Reward: 0, TD Loss: 0.010173996910452843\n",
      "Timestep 2000, Reward: 0, TD Loss: 0.011869436129927635\n",
      "Timestep 2200, Reward: 1, TD Loss: 0.0540001355111599\n",
      "Timestep 2400, Reward: 1, TD Loss: 0.058844201266765594\n",
      "Timestep 2600, Reward: 1, TD Loss: 0.005288479384034872\n",
      "Timestep 2800, Reward: 1, TD Loss: 0.12835945188999176\n",
      "Timestep 3000, Reward: 1, TD Loss: 0.2094477415084839\n",
      "Timestep 3200, Reward: 1, TD Loss: 0.15314817428588867\n",
      "Timestep 3400, Reward: 0, TD Loss: 0.031138813123106956\n",
      "Timestep 3600, Reward: 1, TD Loss: 0.05000942200422287\n",
      "Timestep 3800, Reward: 1, TD Loss: 0.06837743520736694\n",
      "Timestep 4000, Reward: 1, TD Loss: 0.009004473686218262\n",
      "Timestep 4200, Reward: 1, TD Loss: 0.013986299745738506\n",
      "Timestep 4400, Reward: 1, TD Loss: 0.0077495998702943325\n",
      "Timestep 4600, Reward: 1, TD Loss: 0.008029067888855934\n",
      "Timestep 4800, Reward: 1, TD Loss: 0.002155864844098687\n",
      "Timestep 5000, Reward: 1, TD Loss: 0.002661819336935878\n",
      "HPHHP\n",
      "Timestep 200, Reward: 1, TD Loss: 0.0013146477285772562\n",
      "Timestep 400, Reward: -2, TD Loss: 0.0009368080063723028\n",
      "Timestep 600, Reward: -1, TD Loss: 0.0010238370159640908\n",
      "Timestep 800, Reward: 1, TD Loss: 0.0012104385532438755\n",
      "Timestep 1000, Reward: -1, TD Loss: 0.0004729267384391278\n",
      "Timestep 1200, Reward: 1, TD Loss: 0.0007034763693809509\n",
      "Timestep 1400, Reward: 1, TD Loss: 0.0018506614724174142\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-35cd466cbdeb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mseq_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mseq\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_gym\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[0mseq_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mseq\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-46-58db593b616a>\u001b[0m in \u001b[0;36mrun_gym\u001b[1;34m(env)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[1;31m# Update the q-network & the target network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_td_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-1ba755a4915f>\u001b[0m in \u001b[0;36mcompute_td_loss\u001b[1;34m(agent, batch_size, replay_buffer, optimizer, gamma)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# Normal DDQN update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mq_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mq_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# double q-learning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-57ce89f362b9>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLattice2DLinearEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"H\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1368\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1370\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1371\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# Env params\n",
    "collision_penalty = -2\n",
    "trap_penalty = 0.5\n",
    "\n",
    "max_seq_length = 5\n",
    "seq_dict = {}\n",
    "\n",
    "# Run gym on all sequences with length <= max_seq_length\n",
    "for i in range(3, max_seq_length + 1):\n",
    "    print(i)\n",
    "    for j in list(itertools.product('HP', repeat = i)):\n",
    "        seq = ''.join(j)\n",
    "        env = Lattice2DLinearEnv(seq, collision_penalty, trap_penalty)\n",
    "        if i <= 4:\n",
    "            reward, actions = env.all_combs()\n",
    "            seq_dict.update( {seq : reward})\n",
    "        else:\n",
    "            losses, rewards, agent = run_gym(env)\n",
    "            seq_dict.update( {seq : rewards[-1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 200, Reward: 0, TD Loss: 0.5536620020866394\n",
      "Timestep 400, Reward: 0, TD Loss: 0.7131906747817993\n",
      "Timestep 600, Reward: 0, TD Loss: 0.4241718351840973\n",
      "Timestep 800, Reward: 0, TD Loss: 0.48722270131111145\n",
      "Timestep 1000, Reward: 0, TD Loss: 0.3626360595226288\n",
      "Timestep 1200, Reward: 0, TD Loss: 0.136373832821846\n",
      "Timestep 1400, Reward: 0, TD Loss: 0.0877816379070282\n",
      "Timestep 1600, Reward: 0, TD Loss: 0.3731793165206909\n",
      "Timestep 1800, Reward: 0, TD Loss: 0.07666090875864029\n",
      "Timestep 2000, Reward: 0, TD Loss: 0.020956382155418396\n",
      "Timestep 2200, Reward: 0, TD Loss: 0.044152017682790756\n",
      "Timestep 2400, Reward: 0, TD Loss: 0.08546653389930725\n",
      "Timestep 2600, Reward: 0, TD Loss: 0.054331593215465546\n",
      "Timestep 2800, Reward: 0, TD Loss: 0.11670204252004623\n",
      "Timestep 3000, Reward: 0, TD Loss: 0.04263932257890701\n",
      "Timestep 3200, Reward: 1, TD Loss: 0.056685756891965866\n",
      "Timestep 3400, Reward: 1, TD Loss: 0.18817919492721558\n",
      "Timestep 3600, Reward: 0, TD Loss: 0.09477299451828003\n",
      "Timestep 3800, Reward: 1, TD Loss: 0.027965854853391647\n",
      "Timestep 4000, Reward: 1, TD Loss: 0.023576799780130386\n",
      "Timestep 4200, Reward: 1, TD Loss: 0.03572583943605423\n",
      "Timestep 4400, Reward: 1, TD Loss: 0.04831545799970627\n",
      "Timestep 4600, Reward: 1, TD Loss: 0.01869887113571167\n",
      "Timestep 4800, Reward: 1, TD Loss: 0.014921674504876137\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-f6097ced9b48>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLattice2DLinearEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"HPPHHP\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrun_gym\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-78728a0386ef>\u001b[0m in \u001b[0;36mrun_gym\u001b[1;34m(env)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m# print(\"action\", action.cpu())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mreplay_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Deep RL Group\\lattice2d_linear_env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    239\u001b[0m         }\n\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = Lattice2DLinearEnv(\"HPPHHP\")\n",
    "run_gym(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "Reward: 0 | Actions: ['D', 'D', 'D', 'D', 'D']\n"
     ]
    }
   ],
   "source": [
    "env = Lattice2DLinearEnv(\"HHHHHP\")\n",
    "agent = Agent(env, q_network, target_q_network)\n",
    "run_agent(agent, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN with CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lattice2d_cnn_env import Lattice2DCNNEnv\n",
    "\n",
    "class CnnDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(CnnDQN, self).__init__()\n",
    "        self.input_shape = (1,201,201,)\n",
    "        self.num_actions = num_actions\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(self.input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.num_actions),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def feature_size(self):\n",
    "        return self.features(torch.zeros(1, *self.input_shape)).view(1, -1).size(1)\n",
    "\n",
    "env = Lattice2DCNNEnv(\"H\")\n",
    "q_network = CnnDQN(env.observation_space.shape, env.action_space.n)\n",
    "target_q_network = deepcopy(q_network)\n",
    "\n",
    "if USE_CUDA:\n",
    "    q_network = q_network.cuda()\n",
    "    target_q_network = target_q_network.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on different sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Env params\n",
    "collision_penalty = -2\n",
    "trap_penalty = 0.5\n",
    "\n",
    "max_seq_length = 10\n",
    "seq_dict = {}\n",
    "\n",
    "target_network_update_f = 1000\n",
    "num_timesteps = 1000000\n",
    "log_every = 10000\n",
    "\n",
    "# Run gym on all sequences with length <= max_seq_length\n",
    "for i in range(max_seq_length):\n",
    "    for j in list(itertools.product('HP', repeat = i)):\n",
    "        seq = ''.join(j)\n",
    "        env = Lattice2DCNNEnv(seq, collision_penalty, trap_penalty)\n",
    "        if i <= 10:\n",
    "            reward, actions = env.all_combs()\n",
    "            seq_dict.update( {seq : reward})\n",
    "        else:\n",
    "            losses, rewards, agent = run_gym(env)\n",
    "            seq_dict.update( {seq : rewards[-1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Lattice2DCNNEnv(\"HPHPHHPHPPHPHHPPHPH\", collision_penalty, trap_penalty)\n",
    "run_agent(agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
