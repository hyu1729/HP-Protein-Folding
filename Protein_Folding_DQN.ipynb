{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein Folding DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "if USE_CUDA:\n",
    "    print(\"Using GPU: GPU requested and available.\")\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    dtypelong = torch.cuda.LongTensor\n",
    "else:\n",
    "    print(\"NOT Using GPU: GPU not requested or not available.\")\n",
    "    dtype = torch.FloatTensor\n",
    "    dtypelong = torch.LongTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, q_network, target_q_network):\n",
    "        self.env = env\n",
    "        self.q_network = q_network\n",
    "        self.target_q_network = target_q_network\n",
    "        self.num_actions = env.action_space.n\n",
    "\n",
    "    def act(self, state, epsilon):\n",
    "        \"\"\"DQN action - max q-value w/ epsilon greedy exploration.\"\"\"\n",
    "        if random.random() > epsilon:\n",
    "            state = torch.tensor(np.float32(state)).type(dtype).unsqueeze(0)\n",
    "            q_value = self.q_network.forward(state)\n",
    "            return q_value.max(1)[1].data[0]\n",
    "        return torch.tensor(random.randrange(self.env.action_space.n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        state = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(\n",
    "            *random.sample(self.buffer, batch_size)\n",
    "        )\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "replay_size = 5000\n",
    "replay_buffer = ReplayBuffer(replay_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon Greedy Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 500\n",
    "\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([epsilon_by_frame(i) for i in range(10000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Temporal Difference Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss(agent, batch_size, replay_buffer, optimizer, gamma):\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "    state = torch.tensor(np.float32(state)).type(dtype)\n",
    "    next_state = torch.tensor(np.float32(next_state)).type(dtype)\n",
    "    action = torch.tensor(action).type(dtypelong)\n",
    "    reward = torch.tensor(reward).type(dtype)\n",
    "    done = torch.tensor(done).type(dtype)\n",
    "\n",
    "    # Normal DDQN update\n",
    "    q_values = agent.q_network(state)\n",
    "    q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    # double q-learning\n",
    "    online_next_q_values = agent.q_network(next_state)\n",
    "    _, max_indicies = torch.max(online_next_q_values, dim=1)\n",
    "    target_q_values = agent.target_q_network(next_state)\n",
    "    next_q_value = torch.gather(target_q_values, 1, max_indicies.unsqueeze(1))\n",
    "\n",
    "    expected_q_value = reward + gamma * next_q_value.squeeze() * (1 - done)\n",
    "    loss = (q_value - expected_q_value.data.detach()).pow(2).mean()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the target network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(q_network, target_q_network, tau):\n",
    "    for t_param, param in zip(target_q_network.parameters(), q_network.parameters()):\n",
    "        if t_param is param:\n",
    "            continue\n",
    "        new_param = tau * param.data + (1.0 - tau) * t_param.data\n",
    "        t_param.data.copy_(new_param)\n",
    "\n",
    "\n",
    "def hard_update(q_network, target_q_network):\n",
    "    for t_param, param in zip(target_q_network.parameters(), q_network.parameters()):\n",
    "        if t_param is param:\n",
    "            continue\n",
    "        new_param = param.data\n",
    "        t_param.data.copy_(new_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "target_update_rate = 0.1\n",
    "gamma = 0.99\n",
    "target_network_update_f = 100\n",
    "num_timesteps = 5000\n",
    "log_every = 200\n",
    "batch_size = 32\n",
    "\n",
    "def run_gym(env):\n",
    "    \n",
    "    print(env.seq)\n",
    "    \n",
    "    # Q and target Q networks are global paraneters\n",
    "    agent = Agent(env, q_network, target_q_network)\n",
    "    optimizer = optim.Adam(q_network.parameters(), lr = learning_rate)\n",
    "    \n",
    "    losses, all_rewards = [], []\n",
    "    episode_reward = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    for ts in range(1, num_timesteps + 1):\n",
    "        epsilon = epsilon_by_frame(ts)\n",
    "        action = agent.act(state, epsilon)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(int(action.cpu()))\n",
    "            \n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            all_rewards.append(episode_reward)\n",
    "            episode_reward = 0\n",
    "\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            # Update the q-network & the target network\n",
    "            loss = compute_td_loss(agent, batch_size, replay_buffer, optimizer, gamma)\n",
    "            losses.append(loss.data)\n",
    "\n",
    "            if ts % target_network_update_f == 0:\n",
    "                # soft_update(agent.q_network, agent.target_q_network, target_update_rate)\n",
    "                hard_update(agent.q_network, agent.target_q_network)\n",
    "\n",
    "        if ts % log_every == 0:\n",
    "            out_str = \"Timestep {}\".format(ts)\n",
    "            if len(all_rewards) > 0:\n",
    "                out_str += \", Reward: {}\".format(all_rewards[-1])\n",
    "            if len(losses) > 0:\n",
    "                out_str += \", TD Loss: {}\".format(losses[-1])\n",
    "            print(out_str)\n",
    "        \n",
    "    return losses, all_rewards, agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Loss & Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(losses, rewards):\n",
    "    plt.figure(figsize = (20,20))\n",
    "    plt.subplot(211)\n",
    "    plt.title(\"Rewards\")\n",
    "    plt.plot(rewards)\n",
    "    plt.subplot(212)\n",
    "    plt.title(\"Loss\")\n",
    "    plt.plot(losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run trained agent on environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(agent, env):\n",
    "    env = env\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action = agent.act(state, 0)\n",
    "        next_state, reward, done, info = env.step(int(action.cpu()))\n",
    "        # env.render()\n",
    "        print(action)\n",
    "        state=next_state\n",
    "        if done:\n",
    "            print(\"Reward: {} | Actions: {}\".format(reward, info['actions']))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN with Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lattice2d_linear_env import Lattice2DLinearEnv\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(self.input_shape[0], 64), nn.ReLU(), nn.Linear(64, self.num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "env = Lattice2DLinearEnv(\"H\")\n",
    "q_network = DQN(env.observation_space.shape, env.action_space.n)\n",
    "target_q_network = deepcopy(q_network)\n",
    "\n",
    "if USE_CUDA:\n",
    "    q_network = q_network.cuda()\n",
    "    target_q_network = target_q_network.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on different sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Env params\n",
    "collision_penalty = -2\n",
    "trap_penalty = 0.5\n",
    "\n",
    "max_seq_length = 5\n",
    "seq_dict = {}\n",
    "\n",
    "# Run gym on all sequences with length <= max_seq_length\n",
    "for i in range(3, max_seq_length + 1):\n",
    "    print(i)\n",
    "    for j in list(itertools.product('HP', repeat = i)):\n",
    "        seq = ''.join(j)\n",
    "        env = Lattice2DLinearEnv(seq, collision_penalty, trap_penalty)\n",
    "        if i <= 4:\n",
    "            reward, actions = env.all_combs()\n",
    "            seq_dict.update( {seq : reward})\n",
    "        else:\n",
    "            losses, rewards, agent = run_gym(env)\n",
    "            seq_dict.update( {seq : rewards[-1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Lattice2DLinearEnv(\"HPPHHP\")\n",
    "run_gym(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Lattice2DLinearEnv(\"HHHHHP\")\n",
    "agent = Agent(env, q_network, target_q_network)\n",
    "run_agent(agent, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN with CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lattice2d_cnn_env import Lattice2DCNNEnv\n",
    "\n",
    "class CnnDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(CnnDQN, self).__init__()\n",
    "        self.input_shape = (1,201,201,)\n",
    "        self.num_actions = num_actions\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(self.input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.num_actions),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def feature_size(self):\n",
    "        return self.features(torch.zeros(1, *self.input_shape)).view(1, -1).size(1)\n",
    "\n",
    "env = Lattice2DCNNEnv(\"H\")\n",
    "q_network = CnnDQN(env.observation_space.shape, env.action_space.n)\n",
    "target_q_network = deepcopy(q_network)\n",
    "\n",
    "if USE_CUDA:\n",
    "    q_network = q_network.cuda()\n",
    "    target_q_network = target_q_network.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on different sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Env params\n",
    "collision_penalty = -2\n",
    "trap_penalty = 0.5\n",
    "\n",
    "max_seq_length = 10\n",
    "seq_dict = {}\n",
    "\n",
    "target_network_update_f = 1000\n",
    "num_timesteps = 1000000\n",
    "log_every = 10000\n",
    "\n",
    "# Run gym on all sequences with length <= max_seq_length\n",
    "for i in range(max_seq_length):\n",
    "    for j in list(itertools.product('HP', repeat = i)):\n",
    "        seq = ''.join(j)\n",
    "        env = Lattice2DCNNEnv(seq, collision_penalty, trap_penalty)\n",
    "        if i <= 10:\n",
    "            reward, actions = env.all_combs()\n",
    "            seq_dict.update( {seq : reward})\n",
    "        else:\n",
    "            losses, rewards, agent = run_gym(env)\n",
    "            seq_dict.update( {seq : rewards[-1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Lattice2DCNNEnv(\"HPHPHHPHPPHPHHPPHPH\", collision_penalty, trap_penalty)\n",
    "run_agent(agent, env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
