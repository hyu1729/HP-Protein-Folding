{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 127] The specified procedure could not be found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-25c12b27943b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[0mdlls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mth_dll_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'*.dll'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdll\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdlls\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdll\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\ctypes\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 127] The specified procedure could not be found"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from HP2D_Env import HP2D\n",
    "from MCTS import MCTS\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "if USE_CUDA:\n",
    "    print(\"Using GPU: GPU requested and available.\")\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    dtypelong = torch.cuda.LongTensor\n",
    "else:\n",
    "    print(\"NOT Using GPU: GPU not requested or not available.\")\n",
    "    dtype = torch.FloatTensor\n",
    "    dtypelong = torch.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, prob, value):\n",
    "        state = np.expand_dims(state, 0)\n",
    "        self.buffer.append((state, prob, value))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        state, prob, value = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.concatenate(state), prob, value\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "replay_size = 160000\n",
    "replay_buffer = ReplayBuffer(replay_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import DualRes\n",
    "\n",
    "dualres = DualRes(10, 4, USE_CUDA)\n",
    "best_player = deepcopy(dualres)\n",
    "\n",
    "if USE_CUDA:\n",
    "    dualres = dualres.cuda()\n",
    "    best_player = best_player.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlphaLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, pred_v, v, pred_p, p):\n",
    "        value_error = (pred_v - v) ** 2\n",
    "        policy_error = torch.sum((-p * (1e-15 + pred_p).log()))\n",
    "        total_error = (value_error.view(-1) + policy_error).mean()\n",
    "        return total_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(net, batch_size, replay_buffer, optimizer, criterion):\n",
    "    state, p, v = replay_buffer.sample(batch_size)\n",
    "    state = torch.tensor(np.float32(state)).type(dtype)\n",
    "    p = torch.tensor(np.float32(p)).type(dtype)\n",
    "    v = torch.tensor(v).type(dtype)\n",
    "    \n",
    "    pred_p, pred_v = net(state)\n",
    "    loss = criterion(pred_v, v, pred_p, p)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_update(q_network, target_q_network):\n",
    "    for t_param, param in zip(target_q_network.parameters(), q_network.parameters()):\n",
    "        if t_param is param:\n",
    "            continue\n",
    "        new_param = param.data\n",
    "        t_param.data.copy_(new_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('seqs.txt', 'r')\n",
    "seq_list = file.readlines()\n",
    "max_length = 100\n",
    "# Get sequences with length <= max_length\n",
    "seqs = [s for s in seq_list if len(s) == max_length]\n",
    "test_seqs = random.sample(seqs, len(seqs) // 10)\n",
    "train_seqs = [s for s in seqs if s not in test_seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def train():\n",
    "    await asyncio.sleep(5)\n",
    "    criterion = AlphaLoss()\n",
    "    optimizer = optim.SGD(dualres.parameters(), lr = 0.001, momentum = 0.9, weight_decay = 0.0001)\n",
    "    \n",
    "    loss = compute_loss(dualres, batch_size, replay_buffer, optimizer, criterion)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def play(num_iter, num_games):\n",
    "    '''\n",
    "    Plays num_games games with the current network.\n",
    "    For each game, adds len(seq) - 1 data points (s_t, pi_t, z_t) to replay buffer.\n",
    "    '''\n",
    "    s = time.perf_counter()\n",
    "    for g in range(num_games):\n",
    "        seq = random.sample(train_seqs, 1)[0][:-1]\n",
    "        clear_output(wait = True)\n",
    "        print(\"Game {} / {}: {}\".format(g + 1, num_games, seq))\n",
    "        env = HP2D(seq, (10,31,31))\n",
    "        states = []\n",
    "        probs = []\n",
    "        state = env.make_state()\n",
    "        for t in range(len(seq) - 1):\n",
    "            temp = int(t < len(seq) // 10)\n",
    "            mcts = MCTS(env, dualres, num_iter, cpuct = 5)\n",
    "            pi_t = mcts.get_prob(state, temp = temp)\n",
    "            sym = get_syms(state, pi_t)\n",
    "            for s, p in sym:\n",
    "                states.append(s)\n",
    "                probs.append(p)\n",
    "            action = np.random.choice(len(pi_t), p = pi_t)\n",
    "            state = env.next_state(state, action)\n",
    "        reward = env.calc_score(states[-1])\n",
    "        for i in range(len(states)):\n",
    "            replay_buffer.push(states[i], probs[i], reward)\n",
    "    e = time.perf_counter() - s\n",
    "    print('MCTS took {} seconds'.format(e[0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate(num_iter):\n",
    "    '''\n",
    "    Evaluates using 25 games with current network, compare results to hypothetical maximum.\n",
    "    Returns a float between (0, 1): score / hyp_max score\n",
    "    '''\n",
    "    for _ in range(25):\n",
    "        seqs = random.sample(test_seqs, 100)\n",
    "        total = 0\n",
    "        for s in seqs:\n",
    "            env = HP2D(s[:-1], (10,31,31))\n",
    "            state = env.make_state()\n",
    "            for t in range(len(s) - 1):\n",
    "                mcts   = MCTS(env, dualres, num_iter, cpuct)\n",
    "                pi     = mcts.get_prob(state, temp = 0)\n",
    "                action = np.random.choice(len(pi), p = pi)\n",
    "                state  = env.next_state(state, action)    \n",
    "            total += env.calc_score(state) / env.hyp_max()\n",
    "    return total / 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_syms(state, pi):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        board: current board\n",
    "        pi: policy vector of size self.get_prob()\n",
    "    Returns:\n",
    "        symmForms: a list of [(board,pi)] where each tuple is a symmetrical\n",
    "                   form of the board and the corresponding pi vector. This\n",
    "                   is used when training the neural network from examples.\n",
    "    \"\"\"\n",
    "    assert(len(pi) == 4)\n",
    "    l = []\n",
    "    \n",
    "    def rotate(pi, n):\n",
    "        for _ in range(n):\n",
    "            temp = pi\n",
    "            pi = [temp[1], temp[3], temp[0], temp[2]]\n",
    "        return pi\n",
    "    \n",
    "    def flip(pi):\n",
    "        return [pi[3], pi[1], pi[2], pi[0]]\n",
    "        \n",
    "    for i in range(1, 5):\n",
    "        for j in [True, False]:\n",
    "            new_state = np.rot90(state, i, (1, 2))\n",
    "            new_pi = rotate(pi, i)\n",
    "            if j:\n",
    "                new_state = np.fliplr(new_state)\n",
    "                new_pi = flip(new_pi)\n",
    "            l += [(new_state, new_pi)]\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main(num_timesteps, num_games, num_iter, batch_size):\n",
    "    '''\n",
    "    Executing play, train, evaluate asynchronously in parallel.\n",
    "    '''\n",
    "\n",
    "    losses = []\n",
    "    scores = []\n",
    "\n",
    "    for ts in range(1, num_timesteps + 1):\n",
    "        # Data generation\n",
    "        playing = loop.create_task(play(num_iter, num_games))\n",
    "\n",
    "        # Update params\n",
    "        training = loop.create_task(train())\n",
    "        \n",
    "        # Evaluate agent\n",
    "        if ts % 1000 == 0:\n",
    "            evaluating = loop.create_task(evaluate(num_iter))\n",
    "            await asyncio.wait([playing, training, evaluating])\n",
    "            \n",
    "            score = evaluating.result()\n",
    "            scores.append(score)\n",
    "        else:\n",
    "            await asyncio.wait([playing, training])\n",
    "            \n",
    "        loss = training.result()\n",
    "        losses.append(loss)\n",
    "        \n",
    "    return losses, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in range(1):\n",
    "    seq = random.sample(train_seqs, 1)[0][:-1]\n",
    "    clear_output(wait = True)\n",
    "    print(\"Game {} / {}: {}\".format(g + 1, 1, seq))\n",
    "    env = HP2D(seq, (10,31,31))\n",
    "    states = []\n",
    "    probs = []\n",
    "    state = env.make_state()\n",
    "    for t in tqdm(range(len(seq) - 1)):\n",
    "        temp = int(t < len(seq) // 10)\n",
    "        mcts = MCTS(env, dualres, 100, cpuct = 5)\n",
    "        pi_t = mcts.get_prob(state, temp = temp)\n",
    "        sym = get_syms(state, pi_t)\n",
    "        for s, p in sym:\n",
    "            states.append(s)\n",
    "            probs.append(p)\n",
    "        action = np.random.choice(len(pi_t), p = pi_t)\n",
    "        state = env.next_state(state, action)\n",
    "    reward = env.calc_score(states[-1])\n",
    "    for i in range(len(states)):\n",
    "        replay_buffer.push(states[i], probs[i], reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timesteps = 1\n",
    "num_games = 1\n",
    "num_iter = 10\n",
    "batch_size = 1\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "\n",
    "losses, scores = loop.run_until_complete(main(num_timesteps, num_games, num_iter, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
