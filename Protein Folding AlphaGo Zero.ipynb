{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT Using GPU: GPU not requested or not available.\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "if USE_CUDA:\n",
    "    print(\"Using GPU: GPU requested and available.\")\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    dtypelong = torch.cuda.LongTensor\n",
    "else:\n",
    "    print(\"NOT Using GPU: GPU not requested or not available.\")\n",
    "    dtype = torch.FloatTensor\n",
    "    dtypelong = torch.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, prob, value):\n",
    "        state = np.expand_dims(state, 0)\n",
    "        self.buffer.append((state, prob, value))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        state, prob, value = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.concatenate(state), prob, value\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "replay_size = 160000\n",
    "replay_buffer = ReplayBuffer(replay_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import DualRes\n",
    "\n",
    "dualres = DualRes(10, 4, USE_CUDA)\n",
    "best_player = deepcopy(dualres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlphaLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, pred_v, v, pred_p, p):\n",
    "        value_error = (pred_v - v) ** 2\n",
    "        policy_error = torch.sum((-p * (1e-15 + pred_p).log()))\n",
    "        total_error = (value_error.view(-1) + policy_error).mean()\n",
    "        return total_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(net, batch_size, replay_buffer, optimizer, criterion):\n",
    "    state, p, v = replay_buffer.sample(batch_size)\n",
    "    state = torch.tensor(np.float32(state)).type(dtype)\n",
    "    p = torch.tensor(np.float32(p)).type(dtype)\n",
    "    v = torch.tensor(v).type(dtype)\n",
    "    \n",
    "    pred_p, pred_v = net(state)\n",
    "    loss = criterion(pred_v, v, pred_p, p)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_update(q_network, target_q_network):\n",
    "    for t_param, param in zip(target_q_network.parameters(), q_network.parameters()):\n",
    "        if t_param is param:\n",
    "            continue\n",
    "        new_param = param.data\n",
    "        t_param.data.copy_(new_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('seqs.txt', 'r')\n",
    "seq_list = file.readlines()\n",
    "max_length = 20\n",
    "# Get sequences with length <= max_length\n",
    "seqs = [s for s in seq_list if len(s) <= max_length]\n",
    "test_seqs = random.sample(seqs, len(seqs) // 10)\n",
    "train_seqs = [s for s in seqs if s not in test_seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_timesteps, num_games, num_iter):\n",
    "    criterion = AlphaLoss()\n",
    "    optimizer = optim.SGD(dualres.parameters(), lr = 0.001, momentum = 0.9, weight_decay = 0.0001)\n",
    "    \n",
    "    losses = []\n",
    "    batch_size = 256\n",
    "    \n",
    "    for ts in range(1, num_timesteps + 1):\n",
    "        # Data generation\n",
    "        for _ in range(num_games):\n",
    "            # Pick random sequence to play\n",
    "            seq = random.sample(train_seqs, 1)[0][:-1]\n",
    "            \n",
    "            play(seq, best_player, num_iter)\n",
    "        # Update params\n",
    "        loss = compute_loss(dualres, batch_size, replay_buffer, optimizer, criterion)\n",
    "        losses.append(loss)\n",
    "        print(loss)\n",
    "        \n",
    "        if ts % 1000 == 0:\n",
    "            improved = evaluate(dualres, best_player, num_iter)\n",
    "            if improved:\n",
    "                hard_update(dualres, best_player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HP2D_Env import HP2D\n",
    "from MCTS import MCTS\n",
    "\n",
    "def play(seq, net, num_iter):\n",
    "    '''\n",
    "    Plays a game with the best player net and sequence seq.\n",
    "    Adds len(seq) - 1 data points (s_t, pi_t, z_t) to replay buffer.\n",
    "    '''\n",
    "    print(seq)\n",
    "    env = HP2D(seq, (10,31,31))\n",
    "    states = []\n",
    "    probs = []\n",
    "    state = env.make_state()\n",
    "    for t in range(len(seq) - 1):\n",
    "        print(t)\n",
    "        temp = int(t < len(seq) // 10)\n",
    "        mcts = MCTS(env, net, num_iter, cpuct = 5)\n",
    "        pi_t = mcts.get_prob(state, temp = temp)\n",
    "        sym = get_syms(state, pi_t)\n",
    "        for s, p in sym:\n",
    "            states.append(s)\n",
    "            probs.append(p)\n",
    "        action = np.random.choice(len(pi_t), p = pi_t)\n",
    "        state = env.next_state(state, action)\n",
    "    reward = env.calc_score(states[-1])\n",
    "    for i in range(len(states)):\n",
    "        replay_buffer.push(states[i], probs[i], reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(curr, best, num_iter):\n",
    "    seqs = random.sample(test_seqs, 100)\n",
    "    ctr = 0\n",
    "    for s in seqs:\n",
    "        env = HP2D(s[:-1], (10,31,31))\n",
    "        state_c = env.make_state()\n",
    "        state_b = env.make_state()\n",
    "        for t in range(len(s) - 1):\n",
    "            mcts_c   = MCTS(env, curr, num_iter, cpuct)\n",
    "            pi_c     = mcts.get_prob(state_c, temp = 0)\n",
    "            action_c = np.random.choice(len(pi_c), p = pi_c)\n",
    "            state_c  = env.next_state(state_c, action)\n",
    "        for t in range(len(s) - 1):\n",
    "            mcts_b   = MCTS(env, best, num_iter, cpuct)\n",
    "            pi_b     = mcts.get_prob(state_b, temp = 0)\n",
    "            action_b = np.random.choice(len(pi_b), p = pi_b)\n",
    "            state_b  = env.next_state(state_b, action)    \n",
    "        if calc_score(state_c) > calc_score(state_b):\n",
    "            ctr += 1\n",
    "    return ctr >= 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_syms(state, pi):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        board: current board\n",
    "        pi: policy vector of size self.get_prob()\n",
    "    Returns:\n",
    "        symmForms: a list of [(board,pi)] where each tuple is a symmetrical\n",
    "                   form of the board and the corresponding pi vector. This\n",
    "                   is used when training the neural network from examples.\n",
    "    \"\"\"\n",
    "    assert(len(pi) == 4)\n",
    "    l = []\n",
    "    \n",
    "    def rotate(pi, n):\n",
    "        for _ in range(n):\n",
    "            temp = pi\n",
    "            pi = [temp[1], temp[3], temp[0], temp[2]]\n",
    "        return pi\n",
    "    \n",
    "    def flip(pi):\n",
    "        return [pi[3], pi[1], pi[2], pi[0]]\n",
    "        \n",
    "    for i in range(1, 5):\n",
    "        for j in [True, False]:\n",
    "            new_state = np.rot90(state, i, (1, 2))\n",
    "            new_pi = rotate(pi, i)\n",
    "            if j:\n",
    "                new_state = np.fliplr(new_state)\n",
    "                new_pi = flip(new_pi)\n",
    "            l += [(new_state, new_pi)]\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHPHPHPHHPP\n",
      "0\n",
      "99.0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "PHHHHPPHPHPPHPHP\n",
      "0\n",
      "99.0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "HHHHPPHHPP\n",
      "0\n",
      "99.0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "HHPPHHHPPHHHPPPP\n",
      "0\n",
      "99.0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "HHHHPPHPHHHP\n",
      "0\n",
      "99.0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "HHPPPPPPHPHHHPPH\n",
      "0\n",
      "99.0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "HHHPHPPPPPHPP\n",
      "0\n",
      "99.0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "PHPPPPPP\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "PHPPHHHHPHHHPPPPHPH\n",
      "0\n",
      "99.0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "HHHPHPHPPP\n",
      "0\n",
      "99.0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "tensor(55.8041, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "train(10000, 9000, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2072"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
